{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugeyZ8bg0ckg",
        "outputId": "69655866-2930-4f14-d875-af0c10566bf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to: /content\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "def unzip_file(zip_path, extract_to=None):\n",
        "    # If no extraction path provided, unzip in the same directory as the zip file\n",
        "    if extract_to is None:\n",
        "        extract_to = os.path.dirname(zip_path)\n",
        "\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_to)\n",
        "            print(f\"Files extracted to: {extract_to}\")\n",
        "    except zipfile.BadZipFile:\n",
        "        print(\"Error: The file is not a zip file or it is corrupted.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Path to the zip file\n",
        "zip_path = '/content/rrl.zip'\n",
        "\n",
        "# Call the function\n",
        "unzip_file(zip_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from ast import literal_eval\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Load the data\n",
        "data_df = pd.read_csv('/content/data (1).csv')\n",
        "data_df['tags_tokenized'] = data_df['tags_tokenized'].apply(literal_eval)\n",
        "\n",
        "# Initialize a dictionary to create the DataFrame for the final structure\n",
        "all_tags = set(tag for tags_list in data_df['tags_tokenized'] for tag in tags_list)\n",
        "data_structure = {tag: [] for tag in all_tags}\n",
        "data_structure['title'] = []\n",
        "\n",
        "# Process each row in the DataFrame\n",
        "for index, row in data_df.iterrows():\n",
        "    # Assuming 'tags_embedding' is a string of floats separated by spaces\n",
        "    embeddings_str = row['tags_embedding']\n",
        "    # Extract the numbers using regex\n",
        "    float_numbers = list(map(float, re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", embeddings_str)))\n",
        "\n",
        "    # Reshape the list into a 2D array, assuming each embedding has a fixed size, e.g., 6\n",
        "    try:\n",
        "        embeddings_array = np.array(float_numbers).reshape(-1, 6)\n",
        "        tag_means = {tag: np.mean(embeddings_array, axis=0).tolist() for tag in row['tags_tokenized']}\n",
        "    except ValueError:\n",
        "        print(f\"Reshaping error with data at index {index}\")\n",
        "        tag_means = {}\n",
        "\n",
        "    # Populate data_structure with tag means or default values\n",
        "    for tag in all_tags:\n",
        "        data_structure[tag].append(tag_means.get(tag, [0]*6))  # Replace [0]*6 with an appropriate default value\n",
        "\n",
        "    data_structure['title'].append(row['title'])\n",
        "\n",
        "# Create the DataFrame\n",
        "final_tags_df = pd.DataFrame(data_structure)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "final_tags_df.to_csv('/content/final_tags.csv', index=False)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/final_tags.csv'\n",
        "data_df = pd.read_csv(file_path)\n",
        "\n",
        "# Process each tag column to replace arrays with their means\n",
        "for column in data_df.columns:\n",
        "    if column != 'title':  # Assuming 'title' is the only non-tag column\n",
        "        # Convert the string representation of list to actual list and calculate mean\n",
        "        data_df[column] = data_df[column].apply(lambda x: np.max(eval(x)))\n",
        "\n",
        "# Save the modified DataFrame back to a CSV file\n",
        "modified_file_path = '/content/max_tags.csv'\n",
        "data_df.to_csv(modified_file_path, index=False)\n",
        "\n",
        "modified_file_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zXCZIiRA1WSg",
        "outputId": "77ccb2d6-78d4-435b-f849-dc6fcdd23ca3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/max_tags.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def process_and_save_facial_features(input_csv_path):\n",
        "    # Load the CSV file\n",
        "    df = pd.read_csv(input_csv_path)\n",
        "\n",
        "    # Remove the 'Frame' column\n",
        "    df['Label']=df['title']\n",
        "    df.drop('title', axis=1, inplace=True)\n",
        "    #df=df.iloc[:, -17:]\n",
        "    # Save the new CSV without the header and the 'Frame' column\n",
        "    updated_csv_path = '/content/facial_features_updated_no_header.csv'\n",
        "    df.to_csv(updated_csv_path, header=False, index=False)\n",
        "\n",
        "    # Create the .info file for the updated facial features dataset\n",
        "    info_content = [f\"{column} continuous\" for column in df.columns if column != 'Label']\n",
        "    info_content.append(\"class discrete\")\n",
        "    info_content.append(\"LABEL_POS -1\")\n",
        "\n",
        "    updated_info_path = '/content/tic-tac-toe.info'\n",
        "    with open(updated_info_path, 'w') as file:\n",
        "        file.write('\\n'.join(info_content))\n",
        "\n",
        "    # Load the updated CSV file to create .data file\n",
        "    data_lines = df.apply(lambda row: ','.join(row.dropna().astype(str)), axis=1).tolist()\n",
        "\n",
        "    # Save the data to a .data file\n",
        "    data_file_path = '/content/tic-tac-toe.data'\n",
        "    with open(data_file_path, 'w') as file:\n",
        "        file.write('\\n'.join(data_lines))\n",
        "\n",
        "    return updated_csv_path, updated_info_path, data_file_path\n",
        "\n",
        "# Example usage\n",
        "input_csv_path = '/content/max_tags.csv'\n",
        "updated_csv_path, updated_info_path, data_file_path = process_and_save_facial_features(input_csv_path)"
      ],
      "metadata": {
        "id": "CIfzu9-b3c1T"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def process_and_save_facial_features(input_csv_path):\n",
        "    # Load the CSV file\n",
        "    df = pd.read_csv(input_csv_path)\n",
        "    print(f\"Initial column count: {len(df.columns)}\")  # Ensure the initial column count is as expected\n",
        "\n",
        "    # Function to adjust the title to exactly three words, separated by commas\n",
        "    def adjust_title(title):\n",
        "        if not isinstance(title, str):\n",
        "            title = \"\"  # Replace non-string values (e.g., NaN) with an empty string\n",
        "        words = title.split()\n",
        "        # If there are fewer than three words, repeat the words to make it three\n",
        "        while len(words) < 3:\n",
        "            words.extend(words[:3-len(words)])  # Ensure loop does not exceed 3 words\n",
        "        return ','.join(words[:3])\n",
        "\n",
        "    # Assuming the last 3 columns are labels\n",
        "    label_columns = [df.columns[-1], df.columns[-2], df.columns[-3]]\n",
        "    for label in label_columns:\n",
        "        df[label] = df[label].astype(str).apply(adjust_title)\n",
        "\n",
        "    print(f\"Column count after label adjustments: {len(df.columns)}\")  # Verify column count remains consistent\n",
        "\n",
        "    # Save the new CSV without the original title columns\n",
        "    updated_csv_path = '/content/facial_features_updated_no_header.csv'\n",
        "    df.to_csv(updated_csv_path, header=False, index=False)\n",
        "\n",
        "    # Create the .info file for the updated facial features dataset\n",
        "    info_content = [f\"{column} continuous\" for column in df.columns if column not in label_columns]\n",
        "    info_content.extend([\"class discrete\"] * 3)  # Assuming three label columns\n",
        "    info_content.append(\"LABEL_POS -3, -2, -1\")  # Indicate positions of labels\n",
        "\n",
        "    updated_info_path = '/content/tic-tac-toe.info'\n",
        "    with open(updated_info_path, 'w') as file:\n",
        "        file.write('\\n'.join(info_content))\n",
        "\n",
        "    # Load the updated CSV file to create .data file\n",
        "    data_lines = df.apply(lambda row: ','.join(row.dropna().astype(str)), axis=1).tolist()\n",
        "\n",
        "    # Save the data to a .data file\n",
        "    data_file_path = '/content/tic-tac-toe.data'\n",
        "    with open(data_file_path, 'w') as file:\n",
        "        file.write('\\n'.join(data_lines))\n",
        "\n",
        "    return updated_csv_path, updated_info_path, data_file_path\n",
        "\n",
        "# Example usage\n",
        "input_csv_path = '/content/max_tags.csv'\n",
        "updated_csv_path, updated_info_path, data_file_path = process_and_save_facial_features(input_csv_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxrkyWGq4kFd",
        "outputId": "bc34e58d-9a81-444e-bd13-372663f74838"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial column count: 330\n",
            "Column count after label adjustments: 330\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trained on the tic-tac-toe data set with NLAFs.\n",
        "%cd /content/rrl\n",
        "!python3 /content/rrl/experiment.py -d /content/tic-tac-toe -bs 16 -s 1@64 -e150 -lrde 200 -lr 0.01 -ki 0 -i 0 -wd 0.0001 --nlaf --alpha 0.45 --beta 3 --gamma 3 --temp 0.001 --print_rule &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcC-lyDA1EJJ",
        "outputId": "1b5dfb13-46a0-47ee-dd58-f27721c1367f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/rrl\n",
            "2024-12-19 17:07:51.919963: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-19 17:07:51.940981: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-19 17:07:51.947395: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-19 17:07:51.961941: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-19 17:07:53.099003: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-12-19 17:07:57.078637: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-19 17:07:57.098889: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-19 17:07:57.104942: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-19 17:07:58.293427: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "D shape: (10, 336)\n",
            "f_df shape (expected columns): (330, 2)\n",
            "Sample f_df contents: 0           cancer\n",
            "1    environmental\n",
            "2        exploding\n",
            "3               vi\n",
            "4          myanmar\n",
            "Name: 0, dtype: object\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/rrl/experiment.py\", line 174, in <module>\n",
            "    train_main(rrl_args)\n",
            "  File \"/content/rrl/experiment.py\", line 167, in train_main\n",
            "    mp.spawn(train_model, nprocs=args.gpus, args=(args,))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/spawn.py\", line 328, in spawn\n",
            "    return start_processes(fn, args, nprocs, join, daemon, start_method=\"spawn\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/spawn.py\", line 284, in start_processes\n",
            "    while not context.join():\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/spawn.py\", line 203, in join\n",
            "    raise ProcessRaisedException(msg, error_index, failed_process.pid)\n",
            "torch.multiprocessing.spawn.ProcessRaisedException: \n",
            "\n",
            "-- Process 0 terminated with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/spawn.py\", line 90, in _wrap\n",
            "    fn(i, *args)\n",
            "  File \"/content/rrl/experiment.py\", line 70, in train_model\n",
            "    db_enc, train_loader, valid_loader, _ = get_data_loader(dataset, args.world_size, rank, args.batch_size,\n",
            "  File \"/content/rrl/experiment.py\", line 23, in get_data_loader\n",
            "    X_df, y_df, f_df, label_pos = read_csv(data_path, info_path, shuffle=True)\n",
            "  File \"/content/rrl/rrl/utils.py\", line 51, in read_csv\n",
            "    raise ValueError(f\"Column count mismatch: Data has {D.shape[1]} columns but {f_df.shape[0]} names provided.\")\n",
            "ValueError: Column count mismatch: Data has 336 columns but 330 names provided.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}